\section{Chronologischer Ablauf}

Dieser Abschnitt beschreibt den praktischen Entwicklungsweg von der ersten lauffähigen Implementierung bis zu dem Stand, der in Kapitel~6 evaluiert wird.
Hierbei wird erklärt, welche Entscheidungen im Projektverlauf getroffen wurden und welche Auswirkungen diese hatten.

Die Entwicklung begann mit der Implementierung der Game-Environment.
Hierbei wurden zuerst nur die Pokervarianten Kuhn Poker und Leduc Hold'em umgesetzt, um sich in das Thema einzuarbeiten.
Es wurde noch kein Wert auf Schnelligkeit des Codes oder Speicheranforderungen gelegt, sondern es ging lediglich darum, Verständnis über den Algorithmus zu erlangen.

Anschließend wurde die erste Version eines CFR Solvers umgesetzt.
Bei der Implementierung wurde sich an der Beschreibung des Papers \cite{zinkevich2007regret} orientiert.
Daraus resultierte ein Algorithmus, der den Spielbaum dynamisch mithilfe der Zustandsfunktionen der Game-Environment aufbaut und rekursiv traversiert.

Als initiale Evaluation wurde ein Nash-Equilibrium Test geschrieben, welcher nur für Kuhn Poker funktioniert, indem er spezifische Strategiewahrscheinlichkeiten extrahiert und diese gegen die bekannten Nash-Equilibrium-Bedingungen prüft \cite{kuhn1951simplified}.

Als nächste Evaluation wurde ein Skript erstellt, das den Erwartungswert einer Strategie im Self-Play berechnet.
Es stellte sich jedoch schnell heraus, dass dies keine aussagekräftige Metrik ist.
Dieser Wert ist einem nur von Nutzen, falls der Erwartungswert des betrachteten Spiels bekannt ist.
Dies ist für Spiele wie Kuhn Poker \cite{kuhn1951simplified} und Leduc Hold'em \cite{berns2024leducStackexchange} der Fall, jedoch sind solche Werte für größere Pokervarianten schwer zu finden.

Im nächsten Schritt wurde zur Verifizierung der Strategiequalität mit der Implementierung des Accelerated-Best-Response-Agents begonnen.
Dies funktionierte für Kuhn Poker sofort, jedoch gab es für Spiele, die öffentliche Zufallsereignisse beinhalten, Probleme.
Diese Probleme sind rückblickend auf simple Fehlentscheidungen in der Struktur der Game-Environment zurückzuführen.

Die initiale Implementierung hatte zwei große Schwächen. 
Zunächst gibt es keinen expliziten Knoten für öffentliche Zufallsereignisse.
Das heißt, es gibt keinen Zustand, in dem die erste Setzrunde beendet ist, aber noch keine öffentliche Board-Karte ausgeteilt wurde.
Die Game-Environment teilt zum Rundenende sofort eine öffentliche Karte aus.
Dies stellt ein Problem dar, da der Public-State-Tree, über den der Best-Response-Agent traversiert, explizite Zufallsknoten benötigt, um zuverlässig funktionieren zu können.

Das zweite Problem ist direkt mit dem ersten verknüpft. 
Da es keine expliziten Zufallsknoten gibt, wird die Verteilung der Karten bereits vor dem Spiel festgelegt.
Dies geschah durch die Klasse \enquote{Combination Generator}, die deterministisch alle möglichen Kartenkombinationen eines Spiels findet und dann vor dem Spiel das Deck in eine spezifische Reihenfolge mischt, damit die Game-Environment nach Rundenende die gewünschte Karte austeilt.
Hierdurch wurde für jede mögliche Kombination also ein eigener Subtree aufgebaut.
Dies ist problematisch, da dadurch bei jeder Iteration unnötig viele Pfade durchlaufen werden müssen, die schon durchlaufen wurden.
Dadurch ist die Anzahl an Knoten der Spiele fehlerhaft, wodurch die Komplexität nicht richtig eingeschätzt werden kann.
Außerdem steigt die Laufzeit pro Iteration unnötig.

Diese Fehler waren zu diesem Zeitpunkt noch nicht erkannt, weshalb zunächst die Dokumentation der Arbeit in den Vordergrund gerückt ist.
Anschließend wurde im Best-Response-Agent und Public State Tree sehr viel Workaround-Code geschrieben, um die vorher genannten Probleme zu umgehen.
Durch diese Workarounds wurde die Evaluation funktionsfähig, war jedoch extrem langsam und schwer auf neue Situationen anzupassen.

Nachdem es dann möglich war, mittels des Best-Response-Agents die Exploitability einer Strategie zu berechnen, fielen einige Unstimmigkeiten im Konvergenzverhalten der Solver auf.
Die Probleme an den Solvern CFR und CFR+ konnten relativ schnell behoben werden.
Jedoch konnte für alle Sampling-basierten Ansätze kein erwartetes Konvergenzverhalten festgestellt werden.
Zu diesem Zeitpunkt wurde vermutet, dass dies an der impliziten Zufallsknoten-Implementierung liege.
Später stellte sich jedoch heraus, dass dies andere Gründe hat.

Da die Solver relativ langsam waren, fiel der Fokus in der folgenden Zeit nicht darauf, die Sampling-Solver zum Laufen zu bringen, sondern die bestehenden CFR- und CFR+ Implementierungen zu beschleunigen.
Deshalb wurde ein Ansatz implementiert, der einen vorher gebauten und gespeicherten Tree verwendet und damit die teuren Spielzustandsfunktionsaufrufe der Game-Environment vermeidet.
Jedoch brachte diese Implementierung bei größeren Spielen erhebliche Speicherprobleme mit sich.
Ab einer gewissen Größe passt der Spielbaum nicht mehr in den Speicher.

Gegen Ende des Projekts wurde die Game-Environment überarbeitet, sodass explizite Zufallsknoten existieren.
Dadurch wurde der gesamte Workaround-Code im Best-Response-Agent obsolet.

Da die Sampling-Solver weiterhin nicht wie erwartet funktionierten, wurde zu diesem Zeitpunkt der DCFR Algorithmus hinzugezogen.

Zusätzlich wurde noch ein neuer Ansatz mit einem statischen Spielbaum implementiert, der eine Layer-basierte Traversierung beinhaltet.
Die Idee hierfür wurde aus den folgenden Papern \cite{kim2024gpu} \cite{reis2015gpu} entnommen.
Dieser Ansatz reduziert die Iterationszeit gegenüber dem rekursiven Verfahren enorm.

Schließlich wurde am Ende des Projekts erkannt, warum die Sampling-Solver sich nicht wie erwartet verhalten.
Sampling-basierte Solver traversieren nur einen zufällig ausgewählten Teil des Spielbaums.
Dadurch ist die Strategie nach einer Iteration nur für die Pfade definiert, die besucht wurden.
Der Best-Response-Agent evaluiert alle möglichen Informationsets eines Spielers.
Wenn für ein Informationset keine Strategie vorhanden ist, fällt er auf eine uniforme Strategie zurück.
Dadurch wird das Ergebnis natürlich stark verfälscht.
Wenn man die Exploitability eines Sampling-basierten Solvers messen will, braucht man einen anderen Ansatz.
Da diese Erkenntnis jedoch zu spät im Projektverlauf gewonnen wurde, werden die Sampling-basierten Ansätze nicht in der Evaluation betrachtet.

Es wurden im Verlauf des Projekts auch Dinge wie Partial Pruning ausprobiert.
Dies ist eine Pruning-Technik, bei der alle Pfade, für die die Erreichbarkeitswahrscheinlichkeit des Gegners null ist, nicht weiter betrachtet werden.
Dies kann erhebliche Laufzeitverbesserungen mit sich bringen, hat aber aufgrund des gleichen Problems wie die Sampling-Solver nicht zu den erwünschten Konvergenzverhalten geführt.

Themenbereiche wie Abstraktion wurden ebenfalls betrachtet, jedoch stellte sich heraus, dass dieser Bereich einen so großen Umfang hat, dass er im Rahmen dieser Arbeit aufgrund von Zeitmangel nicht weiter verfolgt wurde. 