\section{Ausblick}

In diesem Abschnitt werden konkrete nächste Schritte skizziert, um die Implementierung weiter zu skalieren.
Hierbei wird zwischen Implementierungsansätzen und Algorithmischen Ansätzen unterschieden.

\subsection{Implementierungsoptimierungen}
Auf der Implementierungsseite ist ein Ansatz, der einen statischen Spielbaum aufbaut, bei Spielen höherer Komplexität nicht mehr praktikabel.
Daher ist ab einem bestimmten Komplexitätsgrad ein Umstieg auf eine dynamische Implementierung erforderlich.
Um dies praktikabel zu machen, muss die Spielumgebung grundlegend überarbeitet und auf Laufzeiteffizienz optimiert werden.
Eine weitere Optimierung wäre eine parallelisierte Berechnung auf einem dynamischen Baum.
%Dies könnte zum Beispiel in den unabhängigen Pfaden eines Sampling-Solvers geschehen.
Zusätzlich wäre es sinnvoll, einen Sampling-basierten Ansatz zu verwenden, sobald ein dynamischer Ansatz verfolgt wird.
Hierfür müsste eine passende Evaluation implementiert werden.

Ein weiterer interessanter Ansatz wäre die Verwendung anderer Programmiersprachen wie C++ oder Rust.
Da diese Sprachen näher an der Maschinensprache sind, könnte damit eine erhebliche Laufzeitverbesserung erreicht werden.
Auf die Speicherprobleme der Ansätze mit statischem Spielbaum hätte dies jedoch voraussichtlich keine Auswirkung.

Die Evaluation mittels des Best Response Agents kann weiter optimiert werden, indem die noch nicht umgesetzten Optimierungen aus \hyperref[sec:exploitability]{Kapitel~2.5} implementiert werden.
Außerdem sollte der Best Response Agent genauso wie die Game-Environment auf effizientere Datenstrukturen umgestellt werden.

Bei einer Layer-basierten Implementierung bietet sich die GPU-Optimierung an, die in \cite{kim2024gpu,reis2015gpu} beschrieben wird.
Die bereits verwendeten NumPy-Arrays könnten dabei durch GPU-Tensoren ersetzt werden, was eine erhebliche Beschleunigung der Berechnungen ermöglichen würde.

\subsection{Algorithmische Optimierungen}
Bei den algorithmischen Optimierungen gibt es zahlreiche Möglichkeiten.

Eine erste Maßnahme wäre die Verwendung von Abstraktionstechniken, um die Größe des Spielbaums zu reduzieren.
Bei der Abstraktion wird eine strategische Approximation des vollständigen Spiels erstellt, indem ähnliche Informationssets zu abstrakten Informationssets zusammengefasst werden.
Dadurch wird die Anzahl der zu betrachtenden Zustände erheblich reduziert.
Dies hilft sowohl allen Tree-basierten Ansätzen, da dadurch der abstrahierte Baum des nächstgrößeren Spiels in den Speicher passen könnte, als auch der Laufzeit pro Iteration, da nicht so viele Zustandsknoten besucht werden müssen.
Relevante Literatur hierzu ist \cite{gilpin2007lossless,billings2003approximating,brown2015hierarchical,kroer2018unified}.

Eine zweite Optimierung wäre das Einbauen verschiedener Arten von Pruning.
Beim Pruning werden bestimmte Teilbäume nicht weiter besucht, sobald bestimmte Umstände eingetroffen sind.
Bei Partial Pruning werden Updates für einen Spieler übersprungen, wenn der Gegner den entsprechenden Knoten mit Wahrscheinlichkeit null erreicht.
Eine weiterentwickelte Variante ist Regret-Based Pruning, bei der Pfade übersprungen werden können, wenn einer der Spieler Aktionen mit Wahrscheinlichkeit null wählt, unabhängig davon, welcher Spieler am Zug ist.
Die Anzahl der Iterationen, während derer eine Aktion gepruned werden kann, hängt dabei vom negativen Regret dieser Aktion ab.
Relevante Literatur hierzu ist \cite{brownSandholm2015regretBasedPruning,brown2017reduced}.

Eine weitere Optimierung wäre die Verwendung von Deep CFR \cite{brown2019deepcfr}.
Deep CFR verwendet Funktionenapproximation mit tiefen neuronalen Netzen, um das Verhalten von tabularem CFR zu approximieren.
Anstatt die Strategie für jedes Informationsset explizit zu speichern, wird sie durch ein neuronales Netz dargestellt.
Dies ermöglicht es, auch für sehr große Spiele Strategien zu speichern, die nicht mehr vollständig im Speicher gehalten werden können.

Eine weitere Optimierung wäre Subgame Solving \cite{brown2017superhuman}\\\cite{brown2019superhuman}.
Im Gegensatz zu perfekt-information games kann bei imperfect-information games ein Teilspiel nicht isoliert gelöst werden, da die optimale Strategie davon abhängt, wie in anderen Teilen des Spiels gespielt wird.
Beim Subgame Solving wird daher zuerst eine grobe Strategie für das gesamte Spiel berechnet.
Während des Spiels wird dann für jedes erreichte Teilspiel eine detailliertere Strategie berechnet, die mit der groben Gesamtstrategie kompatibel ist.


